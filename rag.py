import os
import pandas as pd
from langchain.document_loaders import DataFrameLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
import gradio as gr

openai_api_key = "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"
os.environ['OPENAI_API_KEY'] = "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" # arthurzllu

# 1. è®€å–æª”æ¡ˆ
#df = pd.read_excel("./outlook_search.xlsx")
df = pd.read_excel("./email_data.xlsx")
#df_content = pd.read_csv("./all_train_course_contents.csv")

# å‡è¨­å…©å€‹æª”æ¡ˆéƒ½æœ‰ 'èª²ç¨‹æ•˜è¿°' æ¬„ä½ï¼Œæˆ‘å€‘å…ˆåˆä½µ
#df = pd.merge(df_content, df_enroll, on = "èª²ç¨‹åç¨±", how = "left")



from langchain.schema import Document

documents = []
for _, row in df.iterrows():
    # ä½¿ç”¨æ‰€æœ‰æ¬„ä½ä½œç‚º metadata
    metadata = {col: row[col] for col in df.columns}

    # å°‡æ‰€æœ‰æ¬„ä½çš„å€¼åˆä½µç‚ºæ–‡æœ¬å…§å®¹
    page_content = row['body']

    documents.append(Document(
        page_content=page_content,
        metadata=metadata
    ))

price_dic = {"pricing":{
    "Wireless Keyboard": 40, "Wireless Mouse": 15, "USB-C Hub": 25, "Monitor Stand": 30,
    "Webcam": 50, "Laptop Cooler": 20, "External SSD": 100, "HDMI Cable": 10,
    "Desk Lamp": 35, "Ergonomic Chair": 150, "LED Monitor": 300, "Graphics Card": 400,
    "Mechanical Keyboard": 80, "Gaming Mouse": 45, "Soundbar": 60, "Network Router": 90,
    "Smart Speaker": 50, "Power Strip": 15, "USB Charger": 20, "Cooling Fan": 12}}

page_content = str(price_dic) + "\n\n" + "ç”¢å“åƒ¹æ ¼åˆ—è¡¨ï¼š" + "\n".join([f"{k}: {v}" for k, v in price_dic["pricing"].items()])
#documents.append(Document(page_content=page_content, metadata={"pricing": str(price_dic["pricing"])}))


#print(documents)



# 3. åˆ‡åˆ†æ–‡ä»¶ï¼ˆé¿å…éé•·ï¼‰

from langchain.text_splitter import RecursiveCharacterTextSplitter

chunk_size = 200
chunk_overlap = 20

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = chunk_size,
    chunk_overlap = chunk_overlap,
    separators = ["\n\n", "\n", "(?<=ã€‚)", "(?<=ï¼)", "(?<=ï¼Ÿ)", "", "(?<=.)", "(?<=!)", "(?<=?)"]
    # separators = [ "\n\n", "\n", "(?<=\ã€‚ )", " ", "" ]
)

chunks = text_splitter.split_documents(documents)


# 4. å»ºç«‹ Embedding èˆ‡ VectorStore

embedding = OpenAIEmbeddings(
    model = "text-embedding-3-large", # text-embedding-ada-002,
    openai_api_key = openai_api_key
)

import chromadb
from chromadb.config import Settings

# è¨­å®š ChromaDB
chroma_settings = Settings(
    anonymized_telemetry=False,
    is_persistent=True
)

# åˆªé™¤èˆŠçš„è³‡æ–™åº«ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
import shutil
if os.path.exists("./chroma_db"):
    shutil.rmtree("./chroma_db")

collection_name = "faq"
persist_path = "./chroma_db"

vectordb = Chroma.from_documents(
    documents=chunks,
    embedding=embedding,
    persist_directory=persist_path,
    collection_name=collection_name,
    client_settings=chroma_settings
)

# å°‡å‘é‡å¯«å…¥ç¡¬ç¢Ÿ
vectordb.persist()
vectordb._collection.count()


# 5. æ¸¬è©¦ vector db

query = "uther"
results = vectordb.similarity_search(query, k = 10)

# é¡¯ç¤ºæŸ¥è©¢çµæœ
for doc in results:
    print(f"ğŸ“Œ å¯„ä»¶äºº: {doc.metadata['sender']}")



# 6. å»ºç«‹ LLM

from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

model = ChatOpenAI(model = "gpt-4o", temperature = 0)



# 7. å»ºç«‹ prompt

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
       input_variables=["context", "question"],
       template="""
               ä½ æ˜¯ä¸€ä½éƒµä»¶æŸ¥è©¢å°ˆå®¶ã€‚ä½ çš„å·¥ä½œæ˜¯æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œåœ¨ context ä¸­æ‰¾åˆ°æœ€æ­£ç¢ºçš„å›ç­”ã€‚
               **å¦‚æœ context ä¸­æ²’æœ‰ä¸€æ¨£çš„äººåï¼Œè«‹ç›´æ¥å›ç­” "æˆ‘ä¸çŸ¥é“"**ã€‚
               context ä¸­åŒ…å« pricing ä¿¡æ¯ï¼Œä½æ–¼ 'ç”¢å“åƒ¹æ ¼åˆ—è¡¨ï¼š' ä¹‹å¾Œï¼Œæ ¼å¼ç‚º 'ç”¢å“åç¨±: åƒ¹æ ¼'ã€‚ä½ éœ€è¦è§£æå®ƒä¾†å›ç­”å•é¡Œã€‚
               context ä¸­åŒ…å« éŠ·å”®æ•¸é‡ ä¿¡æ¯ï¼Œä½æ–¼ "æ•¸é‡" ä½ éœ€è¦è§£æå®ƒä¾†å›ç­”å•é¡Œã€‚
               context ä¸­åŒ…å« ç”¢å“åç¨± ä¿¡æ¯ï¼Œä½æ–¼ 'tables_json':"rows" ç¬¬1å€‹å€¼ ä½ éœ€è¦è§£æå®ƒä¾†å›ç­”å•é¡Œã€‚


               Context: {context}
               Question: {question}
               Answer:
           """
   )

# 7. å»ºç«‹ prompt

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
       input_variables=["context", "question"],
       template="""
               ä½ æ˜¯ä¸€ä½éƒµä»¶æŸ¥è©¢å°ˆå®¶ã€‚ä½ çš„å·¥ä½œæ˜¯æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œåœ¨ context ä¸­æ‰¾åˆ°æœ€æ­£ç¢ºçš„å›ç­”ã€‚



               Context: {context}
               Question: {question}
               Answer:
           """
   )

# 8. å»ºç«‹ QA

retriever = vectordb.as_retriever(search_kwargs = {
            "fetch_k" : 800,  #åˆå§‹æª¢ç´¢çš„æ–‡ä»¶æ•¸é‡ (5 ç¯‡)ã€‚
            "k" : 100,    #æœ€çµ‚è¿”å›çš„æ–‡ä»¶æ•¸é‡ (3 ç¯‡)
            "mmr_score_cache" : True,
            "mmr_rerank_top_k" : 3  #MMR é‡æ–°æ’åºçš„å‰å¹¾åæ–‡ä»¶æ•¸é‡ (10 ç¯‡)ã€‚
        }, retriever_mode = "reduce_op_recursive", search_type = "mmr")


chatbot = RetrievalQA.from_chain_type(
            llm = model,
            retriever = retriever,
            return_source_documents = True,
            chain_type = "stuff",
            chain_type_kwargs = { "prompt" : prompt }
        )


class QA():

    def __init__(self):
        self.model = RetrievalQA.from_chain_type(
            llm = model,
            retriever = retriever,
            return_source_documents = True,
            chain_type = "stuff",
            chain_type_kwargs = { "prompt" : prompt }
        )

    def query(self, prompt):
        return self.model(prompt)["result"]

robot = QA()

# å„²å­˜å°è©±æ­·å²
chat_history = []

def qa_bot(user_input):
    global chat_history

    response = robot.query(user_input)

    # å°‡ä½¿ç”¨è€…è¼¸å…¥åŠ å…¥æ­·å²
    chat_history.append({"ä½¿ç”¨è€…": user_input})

    # åŠ å…¥æ©Ÿå™¨äººå›æ‡‰åˆ°å°è©±æ­·å²
    chat_history.append({"æ©Ÿå™¨äºº": response + "\n" + "-" * 20 })

    # æ ¼å¼åŒ–å°è©±æ­·å²è¼¸å‡º
    chat_display = "\n".join(
        [f"ä½¿ç”¨è€…: {item['ä½¿ç”¨è€…']}" if 'ä½¿ç”¨è€…' in item else f"æ©Ÿå™¨äºº: {item['æ©Ÿå™¨äºº']}" for item in chat_history]
    )

    return chat_display

with gr.Blocks() as app:
    gr.Markdown("Email(è¨‚å–®)æŸ¥è©¢æ©Ÿå™¨äºº")
    chatbot = gr.Textbox(label = "å°è©±ç´€éŒ„", interactive = False, lines = 10)
    user_input = gr.Textbox(label = "è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ", placeholder = "åœ¨é€™è£¡è¼¸å…¥æ‚¨çš„å•é¡Œ...")
    submit_button = gr.Button("é€å‡º")
    examples = [ "å“ªå€‹ç”¢å“æœ€å¤šäººè²·?" ]
    submit_button.click(qa_bot, inputs = [user_input], outputs = [chatbot])

app.launch(server_name = "0.0.0.0", server_port = 9001, debug = True)


user_input = "keyboardè³£äº†å¤šå°‘å€‹"
response = chatbot(user_input)
print(response["result"])

user_input = "å“ªå€‹ç”¢å“æœ€å¤šäººè²·"
response = chatbot(user_input)
print(response["result"])

user_input = "å“ªå€‹ç”¢å“æœ€ä¾¿å®œ"
response = chatbot(user_input)
print(response["result"])

user_input = "Benåœ¨4/29æœ‰è²·æ±è¥¿å—? è²·äº†ä»€éº¼?"
response = chatbot(user_input)
print(response["result"])

user_input = "PO-20250427-706å…§å®¹?"
response = chatbot(user_input)
print(response["result"])

user_input = "Rickyåœ¨4/29æœ‰è²·æ±è¥¿å—? è²·äº†ä»€éº¼?"
response = chatbot(user_input)
print(response["result"])